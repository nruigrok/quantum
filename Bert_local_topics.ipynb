{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ec7e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Voorzichtig haalde Clim Backes van het Jocus M...</td>\n",
       "      <td>2501 - Cultuur en media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11 blikken schilden met een geschilderde afbee...</td>\n",
       "      <td>2501 - Cultuur en media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fotoclub Venlo bestaat 100 jaar dit jaar en vi...</td>\n",
       "      <td>2501 - Cultuur en media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>De melding van het ongeval kwam rond 16.00 uur...</td>\n",
       "      <td>2500 - Ongevallen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>De twee Blerickenaren worden op vrijdag 20 jan...</td>\n",
       "      <td>2499 - Jubilea en herdenkingen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Na vanavond is de gemeente Venlo dus drie prin...</td>\n",
       "      <td>2503 - Evenementen en uitgaan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&amp;nbsp;De oud-speler van VVV ontving de waarder...</td>\n",
       "      <td>2499 - Jubilea en herdenkingen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Roemer stond samen met Bart Storcken, Emil Sza...</td>\n",
       "      <td>2503 - Evenementen en uitgaan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>De winst tegen de tegenstander uit Beek beteke...</td>\n",
       "      <td>2495 - Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Elke zaterdag om 11:00 uur live op de radio pr...</td>\n",
       "      <td>2503 - Evenementen en uitgaan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Voorzichtig haalde Clim Backes van het Jocus M...   \n",
       "1  11 blikken schilden met een geschilderde afbee...   \n",
       "2  Fotoclub Venlo bestaat 100 jaar dit jaar en vi...   \n",
       "3  De melding van het ongeval kwam rond 16.00 uur...   \n",
       "4  De twee Blerickenaren worden op vrijdag 20 jan...   \n",
       "5  Na vanavond is de gemeente Venlo dus drie prin...   \n",
       "6  &nbsp;De oud-speler van VVV ontving de waarder...   \n",
       "7  Roemer stond samen met Bart Storcken, Emil Sza...   \n",
       "8  De winst tegen de tegenstander uit Beek beteke...   \n",
       "9  Elke zaterdag om 11:00 uur live op de radio pr...   \n",
       "\n",
       "                            label  \n",
       "0         2501 - Cultuur en media  \n",
       "1         2501 - Cultuur en media  \n",
       "2         2501 - Cultuur en media  \n",
       "3               2500 - Ongevallen  \n",
       "4  2499 - Jubilea en herdenkingen  \n",
       "5   2503 - Evenementen en uitgaan  \n",
       "6  2499 - Jubilea en herdenkingen  \n",
       "7   2503 - Evenementen en uitgaan  \n",
       "8                    2495 - Sport  \n",
       "9   2503 - Evenementen en uitgaan  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d = pd.read_csv(\"data/local_topics.csv\")\n",
    "d[\"label\"] = d[\"Onderwerp\"]\n",
    "d = d[[\"text\", \"label\"]]\n",
    "d.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a189d3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500 - Ongevallen                   28\n",
       "2503 - Evenementen en uitgaan       26\n",
       "2501 - Cultuur en media             23\n",
       "2495 - Sport                        20\n",
       "2502 - Vervoer en werkzaamheden     19\n",
       "2494 - Criminaliteit en justitie    19\n",
       "2497 - Natuur en milieu             15\n",
       "2498 - Bedrijfsleven                15\n",
       "2493 - Gezondheidszorg en corona    11\n",
       "2499 - Jubilea en herdenkingen       8\n",
       "Politiek                             4\n",
       "2496 - Onderwijs                     3\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eddc6c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {1: \"Ongevallen\", 2: \"Evenementen en uitgaan\", 3: \"Cultuur en media\", 4:\"Sport\", 5:\"Vervoer en werkzaamheden\", 6:\"Criminaliteit en justitie\", 7:\"Natuur en milieu\", 8:\"Bedrijfsleven\", 9:\"Gezondheidszorg en corona\", 10: \"Jubilea en herdenkingen\", 11: \"Politiek\", 12:\"Onderwijs\"}\n",
    "label2id = {v:k for (k,v) in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f13c21e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 133\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 58\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "ds = Dataset.from_pandas(d).train_test_split(test_size=.3)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6993505d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 20:21:21.775578: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-22 20:21:22.883402: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-22 20:21:22.883478: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-22 20:21:22.883486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "checkpoint = \"GroNLP/bert-base-dutch-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcc9d99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990575751206487195745523ae6f56ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dfad7415b584c07855a29e1da31c4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 133\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 58\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "tokenized = ds.map(preprocess_function, batched=True)\n",
    "tokenized = tokenized.remove_columns(ds[\"train\"].column_names)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11c30d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9feea369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/nel/.cache/huggingface/hub/models--GroNLP--bert-base-dutch-cased/snapshots/484ff5cec2ad42b434537dadd901d9b8e2b64cd2/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"GroNLP/bert-base-dutch-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"1\": \"Ongevallen\",\n",
      "    \"2\": \"Evenementen en uitgaan\",\n",
      "    \"3\": \"Cultuur en media\",\n",
      "    \"4\": \"Sport\",\n",
      "    \"5\": \"Vervoer en werkzaamheden\",\n",
      "    \"6\": \"Criminaliteit en justitie\",\n",
      "    \"7\": \"Natuur en milieu\",\n",
      "    \"8\": \"Bedrijfsleven\",\n",
      "    \"9\": \"Gezondheidszorg en corona\",\n",
      "    \"10\": \"Jubilea en herdenkingen\",\n",
      "    \"11\": \"Politiek\",\n",
      "    \"12\": \"Onderwijs\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Bedrijfsleven\": 8,\n",
      "    \"Criminaliteit en justitie\": 6,\n",
      "    \"Cultuur en media\": 3,\n",
      "    \"Evenementen en uitgaan\": 2,\n",
      "    \"Gezondheidszorg en corona\": 9,\n",
      "    \"Jubilea en herdenkingen\": 10,\n",
      "    \"Natuur en milieu\": 7,\n",
      "    \"Onderwijs\": 12,\n",
      "    \"Ongevallen\": 1,\n",
      "    \"Politiek\": 11,\n",
      "    \"Sport\": 4,\n",
      "    \"Vervoer en werkzaamheden\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30073\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/nel/.cache/huggingface/hub/models--GroNLP--bert-base-dutch-cased/snapshots/484ff5cec2ad42b434537dadd901d9b8e2b64cd2/pytorch_model.bin\n",
      "Some weights of the model checkpoint at GroNLP/bert-base-dutch-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "   checkpoint, num_labels=12, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fafd2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6921298f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized['train'],\n",
    "    eval_dataset=tokenized['test'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a41577fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 133\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 399\n",
      "  Number of trainable parameters = 109146636\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/letshug/env/lib/python3.8/site-packages/transformers/trainer.py:1527\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1524\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1526\u001b[0m )\n\u001b[0;32m-> 1527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/letshug/env/lib/python3.8/site-packages/transformers/trainer.py:1775\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1773\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1778\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1780\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/letshug/env/lib/python3.8/site-packages/transformers/trainer.py:2523\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2523\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2526\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/letshug/env/lib/python3.8/site-packages/transformers/trainer.py:2568\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2566\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[0;32m-> 2568\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2570\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2571\u001b[0m         )\n\u001b[1;32m   2572\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   2573\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d509efe5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictions \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mtokenized_datasets\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39mshape, predictions\u001b[38;5;241m.\u001b[39mlabel_ids\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f811841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 1\n",
      " 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0\n",
      " 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
      " 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1\n",
      " 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 1 0\n",
      " 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 0\n",
      " 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 0 0\n",
      " 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1\n",
      " 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1\n",
      " 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
      " 0 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1\n",
      " 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1\n",
      " 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1\n",
      " 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1\n",
      " 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1\n",
      " 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1 1 1 1\n",
      " 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0\n",
      " 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1\n",
      " 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1\n",
      " 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92004757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationModule(name: \"glue\", module_type: \"metric\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
      "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
      "Args:\n",
      "    predictions: list of predictions to score.\n",
      "        Each translation should be tokenized into a list of tokens.\n",
      "    references: list of lists of references for each translation.\n",
      "        Each reference should be tokenized into a list of tokens.\n",
      "Returns: depending on the GLUE subset, one or several of:\n",
      "    \"accuracy\": Accuracy\n",
      "    \"f1\": F1 score\n",
      "    \"pearson\": Pearson Correlation\n",
      "    \"spearmanr\": Spearman Correlation\n",
      "    \"matthews_correlation\": Matthew Correlation\n",
      "Examples:\n",
      "\n",
      "    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'accuracy': 1.0}\n",
      "\n",
      "    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'accuracy': 1.0, 'f1': 1.0}\n",
      "\n",
      "    >>> glue_metric = evaluate.load('glue', 'stsb')\n",
      "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
      "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
      "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
      "\n",
      "    >>> glue_metric = evaluate.load('glue', 'cola')\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'matthews_correlation': 1.0}\n",
      "\"\"\", stored examples: 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9002293577981652}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"sst2\")\n",
    "print(metric)\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "513731d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"sst2\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56d19033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file config.json from cache at /home/nel/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/nel/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8b45e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/nel/letshug/env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 67349\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 25257\n",
      "  Number of trainable parameters = 109483778\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25257' max='25257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25257/25257 44:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.269500</td>\n",
       "      <td>0.444317</td>\n",
       "      <td>0.885321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.462873</td>\n",
       "      <td>0.895642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.113200</td>\n",
       "      <td>0.440344</td>\n",
       "      <td>0.900229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test-trainer/checkpoint-500\n",
      "Configuration saved in test-trainer/checkpoint-500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-1000\n",
      "Configuration saved in test-trainer/checkpoint-1000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-1500\n",
      "Configuration saved in test-trainer/checkpoint-1500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-2000\n",
      "Configuration saved in test-trainer/checkpoint-2000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-2500\n",
      "Configuration saved in test-trainer/checkpoint-2500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-3000\n",
      "Configuration saved in test-trainer/checkpoint-3000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-3500\n",
      "Configuration saved in test-trainer/checkpoint-3500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-4000\n",
      "Configuration saved in test-trainer/checkpoint-4000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-4500\n",
      "Configuration saved in test-trainer/checkpoint-4500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-5000\n",
      "Configuration saved in test-trainer/checkpoint-5000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-5500\n",
      "Configuration saved in test-trainer/checkpoint-5500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-6000\n",
      "Configuration saved in test-trainer/checkpoint-6000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-6500\n",
      "Configuration saved in test-trainer/checkpoint-6500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-6500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-7000\n",
      "Configuration saved in test-trainer/checkpoint-7000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-7000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-7500\n",
      "Configuration saved in test-trainer/checkpoint-7500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-7500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-8000\n",
      "Configuration saved in test-trainer/checkpoint-8000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-8000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 872\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-trainer/checkpoint-8500\n",
      "Configuration saved in test-trainer/checkpoint-8500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-8500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-9000\n",
      "Configuration saved in test-trainer/checkpoint-9000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-9000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-9500\n",
      "Configuration saved in test-trainer/checkpoint-9500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-9500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-10000\n",
      "Configuration saved in test-trainer/checkpoint-10000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-10000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-10500\n",
      "Configuration saved in test-trainer/checkpoint-10500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-10500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-11000\n",
      "Configuration saved in test-trainer/checkpoint-11000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-11000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-11500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in test-trainer/checkpoint-11500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-11500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-12000\n",
      "Configuration saved in test-trainer/checkpoint-12000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-12000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-12500\n",
      "Configuration saved in test-trainer/checkpoint-12500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-12500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-13000\n",
      "Configuration saved in test-trainer/checkpoint-13000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-13000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-13500\n",
      "Configuration saved in test-trainer/checkpoint-13500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-13500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-14000\n",
      "Configuration saved in test-trainer/checkpoint-14000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-14000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-14500\n",
      "Configuration saved in test-trainer/checkpoint-14500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-14500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-15000\n",
      "Configuration saved in test-trainer/checkpoint-15000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-15000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-15500\n",
      "Configuration saved in test-trainer/checkpoint-15500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-15500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-16000\n",
      "Configuration saved in test-trainer/checkpoint-16000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-16000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-16500\n",
      "Configuration saved in test-trainer/checkpoint-16500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-16500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 872\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-trainer/checkpoint-17000\n",
      "Configuration saved in test-trainer/checkpoint-17000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-17000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-17500\n",
      "Configuration saved in test-trainer/checkpoint-17500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-17500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-18000\n",
      "Configuration saved in test-trainer/checkpoint-18000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-18000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-18500\n",
      "Configuration saved in test-trainer/checkpoint-18500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-18500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-19000\n",
      "Configuration saved in test-trainer/checkpoint-19000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-19000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-19500\n",
      "Configuration saved in test-trainer/checkpoint-19500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-19500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-20000\n",
      "Configuration saved in test-trainer/checkpoint-20000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-20000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-20500\n",
      "Configuration saved in test-trainer/checkpoint-20500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-20500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-21000\n",
      "Configuration saved in test-trainer/checkpoint-21000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-21000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-21500\n",
      "Configuration saved in test-trainer/checkpoint-21500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-21500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-22000\n",
      "Configuration saved in test-trainer/checkpoint-22000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-22000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-22500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in test-trainer/checkpoint-22500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-22500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-23000\n",
      "Configuration saved in test-trainer/checkpoint-23000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-23000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-23500\n",
      "Configuration saved in test-trainer/checkpoint-23500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-23500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-24000\n",
      "Configuration saved in test-trainer/checkpoint-24000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-24000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-24500\n",
      "Configuration saved in test-trainer/checkpoint-24500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-24500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-25000\n",
      "Configuration saved in test-trainer/checkpoint-25000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-25000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 872\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25257, training_loss=0.20143393338960836, metrics={'train_runtime': 2681.2745, 'train_samples_per_second': 75.355, 'train_steps_per_second': 9.42, 'total_flos': 3088292734504560.0, 'train_loss': 0.20143393338960836, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e952caf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9002293577981652}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
